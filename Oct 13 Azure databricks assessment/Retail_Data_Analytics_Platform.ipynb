{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retail Data Analytics Platform \u2014 Simplified Databricks Version\n",
        "\n",
        "This notebook builds a Bronze \u2192 Silver \u2192 Gold pipeline using existing files in `/FileStore/tables/`.\n",
        "\n",
        "Tables are registered under the schema **`retail_demo`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths and setup\n",
        "RAW_PATH = '/FileStore/tables/'\n",
        "DELTA_BASE = '/dbfs/FileStore/delta/retail_project/'\n",
        "BRONZE_PATH = DELTA_BASE + 'bronze/'\n",
        "SILVER_PATH = DELTA_BASE + 'silver/'\n",
        "GOLD_PATH = DELTA_BASE + 'gold/'\n",
        "\n",
        "spark.sql('CREATE DATABASE IF NOT EXISTS retail_demo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Bronze Layer \u2014 Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customers_df = spark.read.option('header', True).csv(RAW_PATH + 'customers.csv')\n",
        "orders_day1_df = spark.read.option('header', True).csv(RAW_PATH + 'orders_day1.csv')\n",
        "products_df = spark.read.option('multiLine', True).json(RAW_PATH + 'products.json')\n",
        "\n",
        "customers_df.write.format('delta').mode('overwrite').save(BRONZE_PATH + 'customers')\n",
        "orders_day1_df.write.format('delta').mode('overwrite').save(BRONZE_PATH + 'orders')\n",
        "products_df.write.format('delta').mode('overwrite').save(BRONZE_PATH + 'products')\n",
        "\n",
        "spark.sql(\"DROP TABLE IF EXISTS retail_demo.bronze_customers\")\n",
        "spark.sql(f\"CREATE TABLE retail_demo.bronze_customers USING DELTA LOCATION '{BRONZE_PATH}customers'\")\n",
        "spark.sql(\"DROP TABLE IF EXISTS retail_demo.bronze_orders\")\n",
        "spark.sql(f\"CREATE TABLE retail_demo.bronze_orders USING DELTA LOCATION '{BRONZE_PATH}orders'\")\n",
        "spark.sql(\"DROP TABLE IF EXISTS retail_demo.bronze_products\")\n",
        "spark.sql(f\"CREATE TABLE retail_demo.bronze_products USING DELTA LOCATION '{BRONZE_PATH}products'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Silver Layer \u2014 Cleansing & Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "customers = spark.read.format('delta').load(BRONZE_PATH + 'customers')\n",
        "orders = spark.read.format('delta').load(BRONZE_PATH + 'orders')\n",
        "products = spark.read.format('delta').load(BRONZE_PATH + 'products')\n",
        "\n",
        "orders = orders.withColumn('quantity', col('quantity').cast('int'))\\\n",
        "               .withColumn('price', col('price').cast('double'))\\\n",
        "               .filter(col('status') == 'Completed')\\\n",
        "               .withColumn('total_amount', col('quantity') * col('price'))\n",
        "\n",
        "silver = orders.join(customers, 'customer_id', 'left')\\\n",
        "               .join(products, orders.product == products.product_name, 'left')\\\n",
        "               .select('order_id','customer_id','name','region','email','product','quantity','price','total_amount','status','order_date','product_id','category')\n",
        "\n",
        "silver.write.format('delta').mode('overwrite').save(SILVER_PATH + 'orders')\n",
        "spark.sql(\"DROP TABLE IF EXISTS retail_demo.silver_orders\")\n",
        "spark.sql(f\"CREATE TABLE retail_demo.silver_orders USING DELTA LOCATION '{SILVER_PATH}orders'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Gold Layer \u2014 Aggregations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import sum as _sum, row_number, desc\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "silver_df = spark.read.format('delta').load(SILVER_PATH + 'orders')\n",
        "\n",
        "revenue_by_region = silver_df.groupBy('region').agg(_sum('total_amount').alias('total_revenue'))\n",
        "revenue_by_region.write.format('delta').mode('overwrite').save(GOLD_PATH + 'revenue_by_region')\n",
        "\n",
        "product_sales = silver_df.groupBy('product','product_id','category').agg(_sum('total_amount').alias('revenue'))\n",
        "ranked = product_sales.withColumn('rank', row_number().over(Window.orderBy(desc('revenue'))))\n",
        "ranked.write.format('delta').mode('overwrite').save(GOLD_PATH + 'sales_summary')\n",
        "\n",
        "spark.sql(\"DROP TABLE IF EXISTS retail_demo.gold_sales_summary\")\n",
        "spark.sql(f\"CREATE TABLE retail_demo.gold_sales_summary USING DELTA LOCATION '{GOLD_PATH}sales_summary'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Incremental Load (MERGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from delta.tables import DeltaTable\n",
        "\n",
        "orders_day2 = spark.read.option('header', True).csv(RAW_PATH + 'orders_day2.csv')\\\n",
        "               .withColumn('quantity', col('quantity').cast('int'))\\\n",
        "               .withColumn('price', col('price').cast('double'))\\\n",
        "               .filter(col('status') == 'Completed')\\\n",
        "               .withColumn('total_amount', col('quantity') * col('price'))\n",
        "\n",
        "incoming = orders_day2.join(customers, 'customer_id', 'left')\\\n",
        "                       .join(products, orders_day2.product == products.product_name, 'left')\\\n",
        "                       .select('order_id','customer_id','name','region','email','product','quantity','price','total_amount','status','order_date','product_id','category')\n",
        "\n",
        "silver_delta = DeltaTable.forPath(spark, SILVER_PATH + 'orders')\n",
        "\n",
        "silver_delta.alias('t').merge(\n",
        "    incoming.alias('s'),\n",
        "    't.order_id = s.order_id'\n",
        ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Time Travel & VACUUM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from delta.tables import DeltaTable\n",
        "\n",
        "dt = DeltaTable.forPath(spark, GOLD_PATH + 'sales_summary')\n",
        "display(dt.history())\n",
        "\n",
        "# Example: query version 0\n",
        "old_version = spark.read.format('delta').option('versionAsOf', 0).load(GOLD_PATH + 'sales_summary')\n",
        "display(old_version)\n",
        "\n",
        "# VACUUM (for demo only)\n",
        "# spark.sql('SET spark.databricks.delta.retentionDurationCheck.enabled = false')\n",
        "# spark.sql(f\"VACUUM delta.`{GOLD_PATH}sales_summary` RETAIN 0 HOURS\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}